{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/stfgrz/ESS_electives_report/blob/main/ESS_electives.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# ESS Electives Survey Analysis\n",
        "## Comprehensive Publication-Ready Analysis of Student Feedback\n",
        "\n",
        "This notebook provides a thorough, publication-ready analysis of the ESS electives survey data.\n",
        "\n",
        "### Features:\n",
        "- \u2705 Comprehensive data quality validation\n",
        "- \u2705 Descriptive and inferential statistics\n",
        "- \u2705 10+ publication-quality visualizations\n",
        "- \u2705 Correlation and comparative analysis  \n",
        "- \u2705 Statistical significance testing\n",
        "- \u2705 Key insights and recommendations\n",
        "- \u2705 Text analysis of open-ended feedback\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd1 Table of Contents\n",
        "1. [Executive Summary](#executive-summary)\n",
        "2. [Setup](#setup)\n",
        "3. [Data Loading and Quality](#data-loading)\n",
        "4. [Data Processing](#data-processing)\n",
        "5. [Descriptive Statistics](#descriptive-stats)\n",
        "6. [Statistical Analysis](#statistical-analysis)\n",
        "7. [Comprehensive Visualizations](#visualizations)\n",
        "8. [Key Insights](#key-insights)\n",
        "9. [Text Analysis](#text-analysis)\n",
        "10. [Recommendations](#recommendations)\n",
        "11. [Exports](#exports)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"executive-summary\"></a>\n",
        "## 1. Executive Summary\n",
        "\n",
        "This section will present key findings after analysis completion.\n",
        "\n",
        "**Quick Stats:**\n",
        "- Analysis Date: December 2025\n",
        "- Survey: ESS Electives Feedback\n",
        "- Metrics: Course Difficulty, Exam Difficulty, Exam Alignment, Re-enrollment Intent\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"setup\"></a>\n",
        "## 2. Setup and Configuration\n",
        "\n",
        "### Package Installation and Environment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and import required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "required_packages = [\"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"scipy\", \"plotly\", \"wordcloud\", \"tabulate\"]\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    print(\"Installing required packages...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + required_packages)\n",
        "    print(\"\u2713 Installation complete\")\n",
        "\n",
        "# Import all libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr, ttest_ind, mannwhitneyu, chi2_contingency\n",
        "import warnings\n",
        "import re\n",
        "from collections import Counter\n",
        "import os\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import wordcloud (optional)\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    WORDCLOUD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WORDCLOUD_AVAILABLE = False\n",
        "    print(\"Note: wordcloud not available for text analysis\")\n",
        "\n",
        "# Configuration Constants\n",
        "DATA_PATH = 'data/ESS_electives_op_DEC2025.csv'\n",
        "OUTPUT_DIR = 'outputs'\n",
        "FIGURE_DIR = 'figures'\n",
        "MIN_RESPONSES = 3  # Minimum responses required for course analysis\n",
        "FIGURE_DPI = 300   # High resolution for publication\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
        "\n",
        "# Configure plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "sns.set_context(\"notebook\", font_scale=1.1)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 22 + \"SETUP COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\u2713 All libraries loaded\")\n",
        "print(f\"\u2713 Output directory: {OUTPUT_DIR}/\")\n",
        "print(f\"\u2713 Figure directory: {FIGURE_DIR}/\")\n",
        "print(f\"\u2713 Configuration: min_responses={MIN_RESPONSES}, DPI={FIGURE_DPI}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"data-loading\"></a>\n",
        "## 3. Data Loading and Quality Assessment\n",
        "\n",
        "### Loading Survey Data\n",
        "\n",
        "The survey CSV has a special structure:\n",
        "- **Row 0:** Question text\n",
        "- **Row 1:** ImportId mappings\n",
        "- **Rows 2+:** Actual survey responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the survey data\n",
        "print(f\"Loading data from: {DATA_PATH}\")\n",
        "df_raw = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" \" * 25 + \"DATA OVERVIEW\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total rows (with metadata):       {len(df_raw):>6}\")\n",
        "print(f\"Total columns:                    {len(df_raw.columns):>6}\")\n",
        "print(f\"Data rows (actual responses):     {len(df_raw) - 2:>6}\")\n",
        "\n",
        "# Identify metadata and data sections\n",
        "questions_row = df_raw.iloc[0]\n",
        "importid_row = df_raw.iloc[1]\n",
        "data_df = df_raw.iloc[2:].copy()\n",
        "\n",
        "# Analyze columns\n",
        "standard_cols = [c for c in df_raw.columns if not c.startswith('Q') and c not in ['A', 'B', 'C1', 'C2']]\n",
        "question_cols = [c for c in df_raw.columns if c.startswith('Q') or c in ['A', 'B', 'C1', 'C2']]\n",
        "\n",
        "print(f\"\\nColumn types:\")\n",
        "print(f\"  - Standard metadata columns:    {len(standard_cols):>6}\")\n",
        "print(f\"  - Survey question columns:      {len(question_cols):>6}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Quality Assessment\n",
        "\n",
        "Comprehensive quality checks and filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \" * 20 + \"DATA QUALITY REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Check for preview/test responses\n",
        "preview_count = len(data_df[data_df['Status'] == 'Survey Preview'])\n",
        "print(f\"\\n1. Preview/Test Responses:        {preview_count:>6}\")\n",
        "\n",
        "# 2. Completion status\n",
        "finished_count = len(data_df[data_df['Finished'] == 'True'])\n",
        "incomplete_count = len(data_df) - finished_count\n",
        "print(f\"\\n2. Survey Completion:\")\n",
        "print(f\"   - Finished surveys:             {finished_count:>6}  ({finished_count/len(data_df)*100:>5.1f}%)\")\n",
        "print(f\"   - Incomplete surveys:           {incomplete_count:>6}  ({incomplete_count/len(data_df)*100:>5.1f}%)\")\n",
        "\n",
        "# 3. Progress statistics\n",
        "if 'Progress' in data_df.columns:\n",
        "    data_df['Progress_Numeric'] = pd.to_numeric(data_df['Progress'], errors='coerce')\n",
        "    mean_progress = data_df['Progress_Numeric'].mean()\n",
        "    median_progress = data_df['Progress_Numeric'].median()\n",
        "    complete_100 = len(data_df[data_df['Progress_Numeric'] == 100])\n",
        "    \n",
        "    print(f\"\\n3. Progress Distribution:\")\n",
        "    print(f\"   - Mean progress:                {mean_progress:>5.1f}%\")\n",
        "    print(f\"   - Median progress:              {median_progress:>5.1f}%\")\n",
        "    print(f\"   - 100% complete:                {complete_100:>6}\")\n",
        "\n",
        "# 4. Check for duplicates\n",
        "duplicate_count = data_df.duplicated(subset=['ResponseId']).sum()\n",
        "print(f\"\\n4. Duplicate Response IDs:        {duplicate_count:>6}\")\n",
        "\n",
        "# 5. Sample missing data analysis\n",
        "print(f\"\\n5. Missing Data Preview (first 10 question columns):\")\n",
        "sample_cols = question_cols[:10]\n",
        "missing_info = []\n",
        "for col in sample_cols:\n",
        "    if col in data_df.columns:\n",
        "        missing_pct = (data_df[col].isna().sum() / len(data_df)) * 100\n",
        "        missing_info.append((col, missing_pct))\n",
        "\n",
        "for col, pct in sorted(missing_info, key=lambda x: x[1], reverse=True)[:5]:\n",
        "    print(f\"   - {col:25s}: {pct:>5.1f}% missing\")\n",
        "\n",
        "# Apply filtering\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FILTERING CRITERIA:\")\n",
        "print(\"  \u2713 Exclude preview/test responses\")\n",
        "print(\"  \u2713 Include only finished surveys\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_filtered = data_df[\n",
        "    (data_df['Status'] != 'Survey Preview') & \n",
        "    (data_df['Finished'] == 'True')\n",
        "].copy()\n",
        "\n",
        "print(f\"\\n\u2713 Filtered dataset: {len(df_filtered)} valid responses\")\n",
        "print(f\"  (Excluded: {len(data_df) - len(df_filtered)} responses)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"data-processing\"></a>\n",
        "## 4. Data Processing and Transformation\n",
        "\n",
        "### Building Course Mapping\n",
        "\n",
        "Extracting course information from question text using regex patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_course_name(text):\n",
        "    \"\"\"\n",
        "    Extract course name from question text using regex patterns.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Question text\n",
        "        \n",
        "    Returns:\n",
        "        str or None: Course name if found\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    \n",
        "    # Pattern 1: \"about <Course>? -\"\n",
        "    match = re.search(r\"about\\s+(.*?)\\?\\s+-\", text)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    \n",
        "    # Pattern 2: \"Would you enrol in <Course> again?\"\n",
        "    match = re.search(r\"Would you enrol in\\s+(.*?)( again)?\\??$\", text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    \n",
        "    # Pattern 3: \"exam for <Course>?\"\n",
        "    match = re.search(r\"exam for\\s+(.*?)\\??$\", text)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    \n",
        "    return None\n",
        "\n",
        "def determine_question_type(text):\n",
        "    \"\"\"\n",
        "    Determine question type from question text.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Question text\n",
        "        \n",
        "    Returns:\n",
        "        str or None: Question type if identified\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    \n",
        "    # Match question types\n",
        "    if \"How difficult was the course\" in text or \"How hard was the course\" in text:\n",
        "        return \"Course Difficulty\"\n",
        "    elif \"How difficult was the exam\" in text or \"How hard was the exam\" in text:\n",
        "        return \"Exam Difficulty\"\n",
        "    elif \"exam was in line with what has been explained\" in text:\n",
        "        return \"Exam Alignment\"\n",
        "    elif \"Would you enrol in\" in text:\n",
        "        return \"Enrol Again\"\n",
        "    elif \"When did you sit the exam\" in text:\n",
        "        return \"Exam Session\"\n",
        "    elif \"What did you appreciate about\" in text:\n",
        "        return \"Positive Feedback\"\n",
        "    elif \"What could be improved about\" in text:\n",
        "        return \"Improvement Feedback\"\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Build course mapping dictionary\n",
        "print(\"Building course mapping from question texts...\")\n",
        "course_map = {}\n",
        "\n",
        "for col in df_raw.columns:\n",
        "    q_text = questions_row[col]\n",
        "    course_name = extract_course_name(q_text)\n",
        "    q_type = determine_question_type(q_text)\n",
        "    \n",
        "    if course_name and q_type:\n",
        "        if course_name not in course_map:\n",
        "            course_map[course_name] = {}\n",
        "        course_map[course_name][q_type] = col\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"\u2713 Course mapping complete\")\n",
        "print(f\"\\nCourses identified:               {len(course_map):>6}\")\n",
        "print(f\"\\nSample mappings (first 3 courses):\")\n",
        "for i, (course, cols) in enumerate(list(course_map.items())[:3]):\n",
        "    print(f\"\\n  {i+1}. {course}\")\n",
        "    for q_type, col in cols.items():\n",
        "        print(f\"     - {q_type:20s}: {col}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Export data dictionary\n",
        "data_dict = pd.DataFrame([\n",
        "    {'Course': course, 'Question_Type': q_type, 'Column': col}\n",
        "    for course, questions in course_map.items()\n",
        "    for q_type, col in questions.items()\n",
        "])\n",
        "data_dict_path = f'{OUTPUT_DIR}/data_dictionary.csv'\n",
        "data_dict.to_csv(data_dict_path, index=False)\n",
        "print(f\"\\n\u2713 Data dictionary saved to: {data_dict_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Response Scale Mappings\n",
        "\n",
        "Defining mappings to convert text responses to numeric scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define scale mappings\n",
        "difficulty_map = {\n",
        "    'Easy': 1,\n",
        "    'Somewhat easy': 2,\n",
        "    'Neither easy nor difficult': 3,\n",
        "    'Somewhat difficult': 4,\n",
        "    'Difficult': 5\n",
        "}\n",
        "\n",
        "agreement_map = {\n",
        "    'Strongly disagree': 1,\n",
        "    'Somewhat disagree': 2,\n",
        "    'Neither agree nor disagree': 3,\n",
        "    'Somewhat agree': 4,\n",
        "    'Strongly agree': 5\n",
        "}\n",
        "\n",
        "enrol_again_map = {\n",
        "    'Yes': 3,\n",
        "    'No, but I would enrol in the module next year': 2,\n",
        "    'No, I would not enrol in the module again': 1\n",
        "}\n",
        "\n",
        "def clean_enrol_text(text):\n",
        "    \"\"\"Simplify enrolment text for analysis.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"No Response\"\n",
        "    if \"Yes\" in text:\n",
        "        return \"Yes\"\n",
        "    if \"No, but\" in text:\n",
        "        return \"No (Next Year)\"\n",
        "    if \"No, I would not\" in text:\n",
        "        return \"No (Never)\"\n",
        "    return \"Other\"\n",
        "\n",
        "print(\"Scale mappings defined:\\n\")\n",
        "print(\"1. Difficulty Scale (1-5):\")\n",
        "for k, v in difficulty_map.items():\n",
        "    print(f\"   {v}: {k}\")\n",
        "\n",
        "print(\"\\n2. Agreement Scale (1-5):\")\n",
        "for k, v in agreement_map.items():\n",
        "    print(f\"   {v}: {k}\")\n",
        "\n",
        "print(\"\\n3. Enrolment Intent (1-3):\")\n",
        "for k, v in enrol_again_map.items():\n",
        "    print(f\"   {v}: {k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform to Long Format\n",
        "\n",
        "Converting wide-format data to long format for easier analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_get(row, col_name):\n",
        "    \"\"\"Safely retrieve column value from row.\"\"\"\n",
        "    if col_name and col_name in row.index:\n",
        "        return row[col_name]\n",
        "    return None\n",
        "\n",
        "# Transform data to long format\n",
        "print(\"Transforming data to long format...\")\n",
        "records = []\n",
        "\n",
        "for idx, row in df_filtered.iterrows():\n",
        "    respondent_id = row['ResponseId']\n",
        "    \n",
        "    for course, cols in course_map.items():\n",
        "        # Check if respondent reviewed this course\n",
        "        c_diff_col = cols.get('Course Difficulty')\n",
        "        if c_diff_col and pd.notna(safe_get(row, c_diff_col)):\n",
        "            # Extract all course-related responses\n",
        "            c_diff_text = safe_get(row, cols.get('Course Difficulty'))\n",
        "            e_diff_text = safe_get(row, cols.get('Exam Difficulty'))\n",
        "            e_align_text = safe_get(row, cols.get('Exam Alignment'))\n",
        "            enrol_text = safe_get(row, cols.get('Enrol Again'))\n",
        "            session_text = safe_get(row, cols.get('Exam Session'))\n",
        "            positive_fb = safe_get(row, cols.get('Positive Feedback'))\n",
        "            improve_fb = safe_get(row, cols.get('Improvement Feedback'))\n",
        "            \n",
        "            # Create record with both text and numeric values\n",
        "            record = {\n",
        "                'RespondentId': respondent_id,\n",
        "                'Course': course,\n",
        "                'Course Difficulty Text': c_diff_text,\n",
        "                'Exam Difficulty Text': e_diff_text,\n",
        "                'Exam Alignment Text': e_align_text,\n",
        "                'Enrol Again Text': enrol_text,\n",
        "                'Exam Session': session_text,\n",
        "                'Positive Feedback': positive_fb,\n",
        "                'Improvement Feedback': improve_fb,\n",
        "                # Numeric conversions\n",
        "                'Course Difficulty': difficulty_map.get(c_diff_text),\n",
        "                'Exam Difficulty': difficulty_map.get(e_diff_text),\n",
        "                'Exam Alignment': agreement_map.get(e_align_text),\n",
        "                'Enrol Again Numeric': enrol_again_map.get(enrol_text),\n",
        "                'Enrol Again Simplified': clean_enrol_text(enrol_text)\n",
        "            }\n",
        "            records.append(record)\n",
        "\n",
        "# Create long-format dataframe\n",
        "df_long = pd.DataFrame(records)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" \" * 18 + \"TRANSFORMATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total course reviews:             {len(df_long):>6}\")\n",
        "print(f\"Unique respondents:               {df_long['RespondentId'].nunique():>6}\")\n",
        "print(f\"Unique courses reviewed:          {df_long['Course'].nunique():>6}\")\n",
        "print(f\"Avg reviews per respondent:       {len(df_long) / df_long['RespondentId'].nunique():>6.1f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save processed data\n",
        "processed_path = f'{OUTPUT_DIR}/processed_course_feedback.csv'\n",
        "df_long.to_csv(processed_path, index=False)\n",
        "print(f\"\\n\u2713 Processed data saved to: {processed_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"descriptive-stats\"></a>\n",
        "## 5. Descriptive Statistics\n",
        "\n",
        "### Overview of Course Feedback Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate comprehensive descriptive statistics\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 18 + \"DESCRIPTIVE STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Response statistics by course\n",
        "course_stats = df_long.groupby('Course').agg({\n",
        "    'Course Difficulty': ['mean', 'std', 'count'],\n",
        "    'Exam Difficulty': ['mean', 'std'],\n",
        "    'Exam Alignment': ['mean', 'std'],\n",
        "    'Enrol Again Numeric': ['mean', 'std']\n",
        "}).round(2)\n",
        "\n",
        "# Flatten column names\n",
        "course_stats.columns = ['_'.join(col).strip() for col in course_stats.columns.values]\n",
        "course_stats = course_stats.reset_index()\n",
        "course_stats.columns = ['Course', 'Course_Diff_Mean', 'Course_Diff_Std', 'Response_Count',\n",
        "                         'Exam_Diff_Mean', 'Exam_Diff_Std', \n",
        "                         'Exam_Align_Mean', 'Exam_Align_Std',\n",
        "                         'Enrol_Again_Mean', 'Enrol_Again_Std']\n",
        "\n",
        "# Filter by minimum responses\n",
        "course_stats_filtered = course_stats[course_stats['Response_Count'] >= MIN_RESPONSES].copy()\n",
        "\n",
        "print(f\"\\nCourses with {MIN_RESPONSES}+ responses:  {len(course_stats_filtered)}\")\n",
        "print(f\"Total courses in dataset:     {len(course_stats)}\")\n",
        "print(f\"\\nTop 10 most reviewed courses:\")\n",
        "print(course_stats.nlargest(10, 'Response_Count')[['Course', 'Response_Count']].to_string(index=False))\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"OVERALL STATISTICS (all courses):\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCourse Difficulty:\")\n",
        "print(f\"  Mean:    {df_long['Course Difficulty'].mean():.2f}  (1=Easy, 5=Difficult)\")\n",
        "print(f\"  Median:  {df_long['Course Difficulty'].median():.2f}\")\n",
        "print(f\"  Std:     {df_long['Course Difficulty'].std():.2f}\")\n",
        "\n",
        "print(f\"\\nExam Difficulty:\")\n",
        "print(f\"  Mean:    {df_long['Exam Difficulty'].mean():.2f}\")\n",
        "print(f\"  Median:  {df_long['Exam Difficulty'].median():.2f}\")\n",
        "print(f\"  Std:     {df_long['Exam Difficulty'].std():.2f}\")\n",
        "\n",
        "print(f\"\\nExam Alignment:\")\n",
        "print(f\"  Mean:    {df_long['Exam Alignment'].mean():.2f}  (1=Disagree, 5=Agree)\")\n",
        "print(f\"  Median:  {df_long['Exam Alignment'].median():.2f}\")\n",
        "print(f\"  Std:     {df_long['Exam Alignment'].std():.2f}\")\n",
        "\n",
        "# Enrolment intentions\n",
        "enrol_dist = df_long['Enrol Again Simplified'].value_counts()\n",
        "print(f\"\\nRe-enrolment Intent:\")\n",
        "for intent, count in enrol_dist.items():\n",
        "    pct = (count / len(df_long)) * 100\n",
        "    print(f\"  {intent:20s}: {count:>4}  ({pct:>5.1f}%)\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save summary statistics\n",
        "course_stats.to_csv(f'{OUTPUT_DIR}/course_summary_statistics.csv', index=False)\n",
        "print(f\"\\n\u2713 Summary statistics saved to: {OUTPUT_DIR}/course_summary_statistics.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module Comparison (II A vs II B)\n",
        "\n",
        "Analyzing differences between Module II A and Module II B courses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract module information from course names\n",
        "# Assuming courses are preceded by module name in the metadata\n",
        "# Check for module information in Q1 and Q2 columns\n",
        "\n",
        "if 'A' in df_filtered.columns and 'B' in df_filtered.columns:\n",
        "    print(\"Analyzing Module distribution...\")\n",
        "    \n",
        "    # Create module assignment for each response\n",
        "    module_assignments = []\n",
        "    for idx, row in df_filtered.iterrows():\n",
        "        resp_id = row['ResponseId']\n",
        "        module_a = row.get('A')\n",
        "        module_b = row.get('B')\n",
        "        \n",
        "        if pd.notna(module_a) and 'Module II A' in str(module_a):\n",
        "            module_assignments.append((resp_id, 'Module II A'))\n",
        "        elif pd.notna(module_b) and 'Module II B' in str(module_b):\n",
        "            module_assignments.append((resp_id, 'Module II B'))\n",
        "        else:\n",
        "            module_assignments.append((resp_id, 'Unknown'))\n",
        "    \n",
        "    module_df = pd.DataFrame(module_assignments, columns=['RespondentId', 'Module'])\n",
        "    df_long_with_module = df_long.merge(module_df, on='RespondentId', how='left')\n",
        "    \n",
        "    # Compare modules\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"MODULE COMPARISON:\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    module_comparison = df_long_with_module.groupby('Module').agg({\n",
        "        'Course Difficulty': ['mean', 'std', 'count'],\n",
        "        'Exam Difficulty': ['mean', 'std'],\n",
        "        'Exam Alignment': ['mean', 'std']\n",
        "    }).round(2)\n",
        "    \n",
        "    print(f\"\\n{module_comparison.to_string()}\")\n",
        "    \n",
        "    # Statistical test\n",
        "    module_a_diff = df_long_with_module[df_long_with_module['Module'] == 'Module II A']['Course Difficulty'].dropna()\n",
        "    module_b_diff = df_long_with_module[df_long_with_module['Module'] == 'Module II B']['Course Difficulty'].dropna()\n",
        "    \n",
        "    if len(module_a_diff) > 0 and len(module_b_diff) > 0:\n",
        "        t_stat, p_value = ttest_ind(module_a_diff, module_b_diff, equal_var=False)\n",
        "        print(f\"\\nT-test for Course Difficulty (Module II A vs B):\")\n",
        "        print(f\"  t-statistic: {t_stat:.3f}\")\n",
        "        print(f\"  p-value:     {p_value:.3f}\")\n",
        "        if p_value < 0.05:\n",
        "            print(f\"  \u2192 Statistically significant difference (p < 0.05)\")\n",
        "        else:\n",
        "            print(f\"  \u2192 No significant difference (p >= 0.05)\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"Module information not available in expected columns\")\n",
        "    df_long_with_module = df_long.copy()\n",
        "    df_long_with_module['Module'] = 'Unknown'\n",
        "    \n",
        "# Store for later use\n",
        "df_long = df_long_with_module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"statistical-analysis\"></a>\n",
        "## 6. Statistical Analysis\n",
        "\n",
        "### Correlation Analysis\n",
        "\n",
        "Examining relationships between different metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 20 + \"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate correlations on complete cases\n",
        "metrics_for_corr = df_long[['Course Difficulty', 'Exam Difficulty', 'Exam Alignment', 'Enrol Again Numeric']].dropna()\n",
        "\n",
        "if len(metrics_for_corr) > 0:\n",
        "    # Pearson correlation\n",
        "    corr_matrix = metrics_for_corr.corr()\n",
        "    \n",
        "    print(\"\\nPearson Correlation Matrix:\\n\")\n",
        "    print(corr_matrix.round(3).to_string())\n",
        "    \n",
        "    # Highlight key correlations\n",
        "    print(\"\\nKey Findings:\")\n",
        "    \n",
        "    # Difficulty vs Alignment\n",
        "    diff_align_corr = corr_matrix.loc['Course Difficulty', 'Exam Alignment']\n",
        "    print(f\"  - Course Difficulty vs Exam Alignment: r = {diff_align_corr:.3f}\")\n",
        "    if abs(diff_align_corr) > 0.3:\n",
        "        direction = \"positive\" if diff_align_corr > 0 else \"negative\"\n",
        "        print(f\"    \u2192 Moderate {direction} correlation\")\n",
        "    \n",
        "    # Difficulty vs Enrolment\n",
        "    diff_enrol_corr = corr_matrix.loc['Course Difficulty', 'Enrol Again Numeric']\n",
        "    print(f\"  - Course Difficulty vs Re-enrolment: r = {diff_enrol_corr:.3f}\")\n",
        "    if abs(diff_enrol_corr) > 0.3:\n",
        "        direction = \"positive\" if diff_enrol_corr > 0 else \"negative\"\n",
        "        print(f\"    \u2192 Moderate {direction} correlation\")\n",
        "    \n",
        "    # Alignment vs Enrolment\n",
        "    align_enrol_corr = corr_matrix.loc['Exam Alignment', 'Enrol Again Numeric']\n",
        "    print(f\"  - Exam Alignment vs Re-enrolment: r = {align_enrol_corr:.3f}\")\n",
        "    if abs(align_enrol_corr) > 0.3:\n",
        "        direction = \"positive\" if align_enrol_corr > 0 else \"negative\"\n",
        "        print(f\"    \u2192 Moderate {direction} correlation\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Save correlation matrix\n",
        "    corr_matrix.to_csv(f'{OUTPUT_DIR}/correlation_matrix.csv')\n",
        "    print(f\"\\n\u2713 Correlation matrix saved to: {OUTPUT_DIR}/correlation_matrix.csv\")\n",
        "else:\n",
        "    print(\"\\nInsufficient data for correlation analysis\")\n",
        "    corr_matrix = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identifying Outliers and Polarizing Courses\n",
        "\n",
        "Finding courses with unusual patterns or high variance in opinions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify outliers and polarizing courses\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 18 + \"OUTLIER & VARIANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate variance for each course (courses with sufficient responses)\n",
        "course_variance = df_long.groupby('Course').agg({\n",
        "    'Course Difficulty': ['std', 'count']\n",
        "}).reset_index()\n",
        "course_variance.columns = ['Course', 'Difficulty_Std', 'Count']\n",
        "course_variance = course_variance[course_variance['Count'] >= MIN_RESPONSES]\n",
        "\n",
        "# High variance courses (polarizing opinions)\n",
        "high_variance = course_variance.nlargest(5, 'Difficulty_Std')\n",
        "print(\"\\nMost Polarizing Courses (high variance in difficulty ratings):\")\n",
        "for idx, row in high_variance.iterrows():\n",
        "    print(f\"  - {row['Course'][:50]:50s}  std={row['Difficulty_Std']:.2f}\")\n",
        "\n",
        "# Low variance courses (consistent opinions)\n",
        "low_variance = course_variance.nsmallest(5, 'Difficulty_Std')\n",
        "print(\"\\nMost Consistent Courses (low variance):\")\n",
        "for idx, row in low_variance.iterrows():\n",
        "    print(f\"  - {row['Course'][:50]:50s}  std={row['Difficulty_Std']:.2f}\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"visualizations\"></a>\n",
        "## 7. Comprehensive Visualizations\n",
        "\n",
        "### Publication-Ready Figures\n",
        "\n",
        "Creating high-quality visualizations for analysis and publication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Summary Statistics Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display nicely formatted summary table\n",
        "print(\"Top 15 Courses by Response Count:\\n\")\n",
        "top_courses = course_stats.nlargest(15, 'Response_Count')\n",
        "\n",
        "# Create a formatted display\n",
        "display_cols = ['Course', 'Response_Count', 'Course_Diff_Mean', 'Exam_Diff_Mean', \n",
        "                'Exam_Align_Mean', 'Enrol_Again_Mean']\n",
        "display_df = top_courses[display_cols].copy()\n",
        "display_df.columns = ['Course', 'N', 'Course Diff', 'Exam Diff', 'Exam Align', 'Re-enrol']\n",
        "\n",
        "# Format Course name to fit\n",
        "display_df['Course'] = display_df['Course'].str[:45]\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Save formatted table\n",
        "display_df.to_csv(f'{OUTPUT_DIR}/top_courses_summary.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Response Distribution by Course (Bar Chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bar chart of response counts\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "top_20 = course_stats.nlargest(20, 'Response_Count')\n",
        "courses = [c[:40] for c in top_20['Course']]  # Truncate names\n",
        "counts = top_20['Response_Count'].values\n",
        "\n",
        "plt.barh(range(len(courses)), counts, color=sns.color_palette(\"husl\", len(courses)))\n",
        "plt.yticks(range(len(courses)), courses)\n",
        "plt.xlabel('Number of Responses', fontsize=12)\n",
        "plt.ylabel('Course', fontsize=12)\n",
        "plt.title('Top 20 Courses by Response Count', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{FIGURE_DIR}/01_response_distribution.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "print(f\"\u2713 Figure saved: {FIGURE_DIR}/01_response_distribution.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Course Metrics Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create heatmap of all metrics for top courses\n",
        "plt.figure(figsize=(10, 12))\n",
        "\n",
        "# Select top courses and prepare data\n",
        "top_courses_list = course_stats.nlargest(20, 'Response_Count')['Course'].tolist()\n",
        "heatmap_data = course_stats[course_stats['Course'].isin(top_courses_list)].copy()\n",
        "heatmap_data = heatmap_data.set_index('Course')[['Course_Diff_Mean', 'Exam_Diff_Mean', \n",
        "                                                   'Exam_Align_Mean', 'Enrol_Again_Mean']]\n",
        "heatmap_data.index = [c[:35] for c in heatmap_data.index]  # Truncate course names\n",
        "heatmap_data.columns = ['Course\\nDifficulty', 'Exam\\nDifficulty', \n",
        "                         'Exam\\nAlignment', 'Re-enrol\\nIntent']\n",
        "\n",
        "# Sort by course difficulty\n",
        "heatmap_data = heatmap_data.sort_values('Course\\nDifficulty', ascending=False)\n",
        "\n",
        "# Create heatmap with annotations\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
        "            linewidths=0.5, cbar_kws={'label': 'Scale (1-5)'})\n",
        "plt.title('Course Metrics Heatmap (Top 20 Courses)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Course', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{FIGURE_DIR}/02_course_metrics_heatmap.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "print(f\"\u2713 Figure saved: {FIGURE_DIR}/02_course_metrics_heatmap.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Course Difficulty Distribution (Box Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plot of course difficulty for top courses\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "top_15_courses = course_stats.nlargest(15, 'Response_Count')['Course'].tolist()\n",
        "df_top = df_long[df_long['Course'].isin(top_15_courses)].copy()\n",
        "df_top['Course_Short'] = df_top['Course'].str[:35]\n",
        "\n",
        "# Calculate median for sorting\n",
        "median_order = df_top.groupby('Course_Short')['Course Difficulty'].median().sort_values().index\n",
        "\n",
        "sns.boxplot(data=df_top, y='Course_Short', x='Course Difficulty', \n",
        "            order=median_order, palette='coolwarm')\n",
        "plt.title('Distribution of Course Difficulty Ratings (Top 15 Courses)', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Difficulty Rating (1=Easy, 5=Difficult)', fontsize=12)\n",
        "plt.ylabel('Course', fontsize=12)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{FIGURE_DIR}/03_course_difficulty_boxplot.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "print(f\"\u2713 Figure saved: {FIGURE_DIR}/03_course_difficulty_boxplot.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Difficulty vs Alignment Scatter Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot with bubble sizes\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Use course stats for plotting\n",
        "plot_data = course_stats[course_stats['Response_Count'] >= MIN_RESPONSES].copy()\n",
        "\n",
        "scatter = plt.scatter(plot_data['Course_Diff_Mean'], \n",
        "                     plot_data['Exam_Align_Mean'],\n",
        "                     s=plot_data['Response_Count']*20,\n",
        "                     alpha=0.6,\n",
        "                     c=plot_data['Course_Diff_Mean'],\n",
        "                     cmap='coolwarm',\n",
        "                     edgecolors='black',\n",
        "                     linewidth=0.5)\n",
        "\n",
        "# Add labels for selected courses (highest/lowest in each dimension)\n",
        "# Top 3 most difficult\n",
        "top_diff = plot_data.nlargest(3, 'Course_Diff_Mean')\n",
        "for _, row in top_diff.iterrows():\n",
        "    plt.annotate(row['Course'][:25], \n",
        "                xy=(row['Course_Diff_Mean'], row['Exam_Align_Mean']),\n",
        "                xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=8, alpha=0.7)\n",
        "\n",
        "# Top 3 lowest alignment\n",
        "low_align = plot_data.nsmallest(3, 'Exam_Align_Mean')\n",
        "for _, row in low_align.iterrows():\n",
        "    plt.annotate(row['Course'][:25],\n",
        "                xy=(row['Course_Diff_Mean'], row['Exam_Align_Mean']),\n",
        "                xytext=(5, -10), textcoords='offset points',\n",
        "                fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.colorbar(scatter, label='Course Difficulty')\n",
        "plt.xlabel('Average Course Difficulty (1-5)', fontsize=12)\n",
        "plt.ylabel('Average Exam Alignment (1-5)', fontsize=12)\n",
        "plt.title('Course Difficulty vs Exam Alignment\\n(Bubble size = Response count)', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{FIGURE_DIR}/04_difficulty_vs_alignment.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "print(f\"\u2713 Figure saved: {FIGURE_DIR}/04_difficulty_vs_alignment.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Correlation Matrix Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "if not corr_matrix.empty:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    # Create heatmap\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1,\n",
        "                cbar_kws={'label': 'Correlation Coefficient'})\n",
        "    \n",
        "    plt.title('Correlation Matrix of Course Metrics', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.savefig(f'{FIGURE_DIR}/05_correlation_heatmap.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "    print(f\"\u2713 Figure saved: {FIGURE_DIR}/05_correlation_heatmap.png\")\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"Skipping correlation heatmap (insufficient data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Re-enrolment Intent Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stacked bar chart for enrolment intent by course\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Get enrolment breakdown for top courses\n",
        "top_courses_enrol = course_stats.nlargest(15, 'Response_Count')['Course'].tolist()\n",
        "enrol_data = df_long[df_long['Course'].isin(top_courses_enrol)].copy()\n",
        "enrol_data['Course_Short'] = enrol_data['Course'].str[:35]\n",
        "\n",
        "# Create pivot table\n",
        "enrol_pivot = enrol_data.groupby(['Course_Short', 'Enrol Again Simplified']).size().unstack(fill_value=0)\n",
        "\n",
        "# Convert to percentages\n",
        "enrol_pct = enrol_pivot.div(enrol_pivot.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Sort by \"Yes\" percentage\n",
        "if 'Yes' in enrol_pct.columns:\n",
        "    enrol_pct = enrol_pct.sort_values('Yes', ascending=True)\n",
        "\n",
        "# Plot stacked horizontal bar\n",
        "enrol_pct.plot(kind='barh', stacked=True, \n",
        "               color=['#ff6b6b', '#ffd93d', '#6bcf7f', '#95a5a6'],\n",
        "               figsize=(14, 10))\n",
        "\n",
        "plt.xlabel('Percentage of Responses (%)', fontsize=12)\n",
        "plt.ylabel('Course', fontsize=12)\n",
        "plt.title('Would Students Re-enrol? (Top 15 Courses)', fontsize=14, fontweight='bold')\n",
        "plt.legend(title='Response', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{FIGURE_DIR}/06_reenrolment_distribution.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "print(f\"\u2713 Figure saved: {FIGURE_DIR}/06_reenrolment_distribution.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Difficulty Distribution (Violin Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Violin plot for difficulty distribution\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Select courses for violin plot\n",
        "top_10_courses = course_stats.nlargest(10, 'Response_Count')['Course'].tolist()\n",
        "df_violin = df_long[df_long['Course'].isin(top_10_courses)].copy()\n",
        "df_violin['Course_Short'] = df_violin['Course'].str[:30]\n",
        "\n",
        "# Sort by median difficulty\n",
        "median_order = df_violin.groupby('Course_Short')['Course Difficulty'].median().sort_values().index\n",
        "\n",
        "sns.violinplot(data=df_violin, y='Course_Short', x='Course Difficulty',\n",
        "               order=median_order, palette='Set2', inner='box')\n",
        "\n",
        "plt.title('Distribution of Course Difficulty (Violin Plot, Top 10 Courses)', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Difficulty Rating (1=Easy, 5=Difficult)', fontsize=12)\n",
        "plt.ylabel('Course', fontsize=12)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f'{FIGURE_DIR}/07_difficulty_violin_plot.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "print(f\"\u2713 Figure saved: {FIGURE_DIR}/07_difficulty_violin_plot.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Module Comparison (if applicable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Module comparison visualization\n",
        "if 'Module' in df_long.columns and df_long['Module'].nunique() > 1:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "    fig.suptitle('Module II A vs Module II B Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Remove \"Unknown\" for cleaner comparison\n",
        "    df_module_clean = df_long[df_long['Module'] != 'Unknown'].copy()\n",
        "    \n",
        "    if len(df_module_clean) > 0:\n",
        "        # Plot 1: Course Difficulty\n",
        "        sns.boxplot(data=df_module_clean, x='Module', y='Course Difficulty', \n",
        "                   ax=axes[0,0], palette='Set2')\n",
        "        axes[0,0].set_title('Course Difficulty by Module')\n",
        "        axes[0,0].set_ylabel('Difficulty (1-5)')\n",
        "        axes[0,0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Plot 2: Exam Difficulty\n",
        "        sns.boxplot(data=df_module_clean, x='Module', y='Exam Difficulty',\n",
        "                   ax=axes[0,1], palette='Set2')\n",
        "        axes[0,1].set_title('Exam Difficulty by Module')\n",
        "        axes[0,1].set_ylabel('Difficulty (1-5)')\n",
        "        axes[0,1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Plot 3: Exam Alignment\n",
        "        sns.boxplot(data=df_module_clean, x='Module', y='Exam Alignment',\n",
        "                   ax=axes[1,0], palette='Set2')\n",
        "        axes[1,0].set_title('Exam Alignment by Module')\n",
        "        axes[1,0].set_ylabel('Alignment (1-5)')\n",
        "        axes[1,0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Plot 4: Re-enrolment Intent\n",
        "        module_enrol = df_module_clean.groupby(['Module', 'Enrol Again Simplified']).size().unstack(fill_value=0)\n",
        "        module_enrol_pct = module_enrol.div(module_enrol.sum(axis=1), axis=0) * 100\n",
        "        module_enrol_pct.plot(kind='bar', stacked=True, ax=axes[1,1],\n",
        "                             color=['#ff6b6b', '#ffd93d', '#6bcf7f', '#95a5a6'])\n",
        "        axes[1,1].set_title('Re-enrolment Intent by Module')\n",
        "        axes[1,1].set_ylabel('Percentage (%)')\n",
        "        axes[1,1].set_xlabel('Module')\n",
        "        axes[1,1].legend(title='Response', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        axes[1,1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{FIGURE_DIR}/08_module_comparison.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "        print(f\"\u2713 Figure saved: {FIGURE_DIR}/08_module_comparison.png\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"Insufficient module data for comparison plots\")\n",
        "        plt.close()\n",
        "else:\n",
        "    print(\"Module comparison not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Pairwise Relationships (Scatter Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create scatter matrix for key metrics\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "metrics_scatter = df_long[['Course Difficulty', 'Exam Difficulty', \n",
        "                            'Exam Alignment', 'Enrol Again Numeric']].dropna()\n",
        "\n",
        "if len(metrics_scatter) > 10:\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    scatter_matrix(metrics_scatter, alpha=0.5, figsize=(12, 12), diagonal='hist',\n",
        "                  hist_kwds={'bins': 20, 'edgecolor': 'black'})\n",
        "    plt.suptitle('Pairwise Relationships Between Metrics', \n",
        "                fontsize=14, fontweight='bold', y=0.995)\n",
        "    \n",
        "    plt.savefig(f'{FIGURE_DIR}/09_scatter_matrix.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "    print(f\"\u2713 Figure saved: {FIGURE_DIR}/09_scatter_matrix.png\")\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"Insufficient data for scatter matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"key-insights\"></a>\n",
        "## 8. Key Insights and Findings\n",
        "\n",
        "### Automated Insight Generation\n",
        "\n",
        "Identifying the most important patterns and trends in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top/Bottom Performers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate key insights automatically\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 22 + \"KEY INSIGHTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Filter for courses with sufficient responses\n",
        "insights_data = course_stats[course_stats['Response_Count'] >= MIN_RESPONSES].copy()\n",
        "\n",
        "# 1. Most/Least Difficult Courses\n",
        "print(\"\\n1. MOST DIFFICULT COURSES (Top 5):\")\n",
        "most_difficult = insights_data.nlargest(5, 'Course_Diff_Mean')\n",
        "for idx, row in most_difficult.iterrows():\n",
        "    print(f\"   - {row['Course'][:50]:50s}  {row['Course_Diff_Mean']:.2f}/5.0  (N={row['Response_Count']:.0f})\")\n",
        "\n",
        "print(\"\\n2. LEAST DIFFICULT COURSES (Top 5):\")\n",
        "least_difficult = insights_data.nsmallest(5, 'Course_Diff_Mean')\n",
        "for idx, row in least_difficult.iterrows():\n",
        "    print(f\"   - {row['Course'][:50]:50s}  {row['Course_Diff_Mean']:.2f}/5.0  (N={row['Response_Count']:.0f})\")\n",
        "\n",
        "# 3. Best/Worst Exam Alignment\n",
        "print(\"\\n3. BEST EXAM ALIGNMENT (Top 5):\")\n",
        "best_alignment = insights_data.nlargest(5, 'Exam_Align_Mean')\n",
        "for idx, row in best_alignment.iterrows():\n",
        "    print(f\"   - {row['Course'][:50]:50s}  {row['Exam_Align_Mean']:.2f}/5.0  (N={row['Response_Count']:.0f})\")\n",
        "\n",
        "print(\"\\n4. WORST EXAM ALIGNMENT (Bottom 5):\")\n",
        "worst_alignment = insights_data.nsmallest(5, 'Exam_Align_Mean')\n",
        "for idx, row in worst_alignment.iterrows():\n",
        "    print(f\"   - {row['Course'][:50]:50s}  {row['Exam_Align_Mean']:.2f}/5.0  (N={row['Response_Count']:.0f})\")\n",
        "\n",
        "# 5. Highest Re-enrolment Intent\n",
        "print(\"\\n5. HIGHEST RE-ENROLMENT INTENT (Top 5):\")\n",
        "highest_reenrol = insights_data.nlargest(5, 'Enrol_Again_Mean')\n",
        "for idx, row in highest_reenrol.iterrows():\n",
        "    print(f\"   - {row['Course'][:50]:50s}  {row['Enrol_Again_Mean']:.2f}/3.0  (N={row['Response_Count']:.0f})\")\n",
        "\n",
        "# 6. Lowest Re-enrolment Intent\n",
        "print(\"\\n6. LOWEST RE-ENROLMENT INTENT (Bottom 5):\")\n",
        "lowest_reenrol = insights_data.nsmallest(5, 'Enrol_Again_Mean')\n",
        "for idx, row in lowest_reenrol.iterrows():\n",
        "    print(f\"   - {row['Course'][:50]:50s}  {row['Enrol_Again_Mean']:.2f}/3.0  (N={row['Response_Count']:.0f})\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Special Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify special course categories\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \" * 18 + \"SPECIAL COURSE CATEGORIES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# \"Hidden Gems\" - High satisfaction (re-enrol), moderate/low difficulty\n",
        "hidden_gems = insights_data[\n",
        "    (insights_data['Enrol_Again_Mean'] >= insights_data['Enrol_Again_Mean'].quantile(0.75)) &\n",
        "    (insights_data['Course_Diff_Mean'] <= insights_data['Course_Diff_Mean'].median())\n",
        "].copy()\n",
        "\n",
        "print(\"\\n7. HIDDEN GEMS (High Re-enrol Intent + Lower Difficulty):\")\n",
        "if len(hidden_gems) > 0:\n",
        "    hidden_gems = hidden_gems.sort_values('Enrol_Again_Mean', ascending=False).head(5)\n",
        "    for idx, row in hidden_gems.iterrows():\n",
        "        print(f\"   - {row['Course'][:40]:40s}  Difficulty: {row['Course_Diff_Mean']:.2f}  Re-enrol: {row['Enrol_Again_Mean']:.2f}\")\n",
        "else:\n",
        "    print(\"   No courses match this criteria\")\n",
        "\n",
        "# \"Challenging but Rewarding\" - High difficulty, high re-enrol\n",
        "challenging_rewarding = insights_data[\n",
        "    (insights_data['Course_Diff_Mean'] >= insights_data['Course_Diff_Mean'].quantile(0.75)) &\n",
        "    (insights_data['Enrol_Again_Mean'] >= insights_data['Enrol_Again_Mean'].quantile(0.75))\n",
        "].copy()\n",
        "\n",
        "print(\"\\n8. CHALLENGING BUT REWARDING (High Difficulty + High Re-enrol Intent):\")\n",
        "if len(challenging_rewarding) > 0:\n",
        "    challenging_rewarding = challenging_rewarding.sort_values('Enrol_Again_Mean', ascending=False).head(5)\n",
        "    for idx, row in challenging_rewarding.iterrows():\n",
        "        print(f\"   - {row['Course'][:40]:40s}  Difficulty: {row['Course_Diff_Mean']:.2f}  Re-enrol: {row['Enrol_Again_Mean']:.2f}\")\n",
        "else:\n",
        "    print(\"   No courses match this criteria\")\n",
        "\n",
        "# \"Needs Attention\" - Low alignment, low re-enrol\n",
        "needs_attention = insights_data[\n",
        "    (insights_data['Exam_Align_Mean'] <= insights_data['Exam_Align_Mean'].quantile(0.25)) &\n",
        "    (insights_data['Enrol_Again_Mean'] <= insights_data['Enrol_Again_Mean'].quantile(0.25))\n",
        "].copy()\n",
        "\n",
        "print(\"\\n9. NEEDS ATTENTION (Low Exam Alignment + Low Re-enrol Intent):\")\n",
        "if len(needs_attention) > 0:\n",
        "    needs_attention = needs_attention.sort_values('Enrol_Again_Mean').head(5)\n",
        "    for idx, row in needs_attention.iterrows():\n",
        "        print(f\"   - {row['Course'][:40]:40s}  Alignment: {row['Exam_Align_Mean']:.2f}  Re-enrol: {row['Enrol_Again_Mean']:.2f}\")\n",
        "else:\n",
        "    print(\"   No courses match this criteria\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save insights\n",
        "insights_summary = {\n",
        "    'Most Difficult': most_difficult[['Course', 'Course_Diff_Mean', 'Response_Count']].to_dict('records'),\n",
        "    'Least Difficult': least_difficult[['Course', 'Course_Diff_Mean', 'Response_Count']].to_dict('records'),\n",
        "    'Best Alignment': best_alignment[['Course', 'Exam_Align_Mean', 'Response_Count']].to_dict('records'),\n",
        "    'Worst Alignment': worst_alignment[['Course', 'Exam_Align_Mean', 'Response_Count']].to_dict('records'),\n",
        "    'Hidden Gems': hidden_gems[['Course', 'Course_Diff_Mean', 'Enrol_Again_Mean']].to_dict('records') if len(hidden_gems) > 0 else [],\n",
        "    'Challenging But Rewarding': challenging_rewarding[['Course', 'Course_Diff_Mean', 'Enrol_Again_Mean']].to_dict('records') if len(challenging_rewarding) > 0 else [],\n",
        "    'Needs Attention': needs_attention[['Course', 'Exam_Align_Mean', 'Enrol_Again_Mean']].to_dict('records') if len(needs_attention) > 0 else []\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(f'{OUTPUT_DIR}/key_insights.json', 'w') as f:\n",
        "    json.dump(insights_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n\u2713 Key insights saved to: {OUTPUT_DIR}/key_insights.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"text-analysis\"></a>\n",
        "## 9. Text Analysis of Open-Ended Feedback\n",
        "\n",
        "### Analyzing Qualitative Responses\n",
        "\n",
        "Examining the open-ended feedback for common themes and patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text analysis of open-ended feedback\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 20 + \"TEXT ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Collect all positive and improvement feedback\n",
        "positive_texts = df_long['Positive Feedback'].dropna().tolist()\n",
        "improvement_texts = df_long['Improvement Feedback'].dropna().tolist()\n",
        "\n",
        "print(f\"\\nPositive feedback responses:      {len(positive_texts)}\")\n",
        "print(f\"Improvement feedback responses:   {len(improvement_texts)}\")\n",
        "\n",
        "# Word frequency analysis\n",
        "def get_word_frequencies(texts, top_n=20):\n",
        "    \"\"\"Extract word frequencies from text list.\"\"\"\n",
        "    from collections import Counter\n",
        "    import re\n",
        "    \n",
        "    # Combine all texts\n",
        "    all_text = ' '.join(texts).lower()\n",
        "    \n",
        "    # Remove common stop words\n",
        "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
        "                  'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
        "                  'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
        "                  'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these',\n",
        "                  'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which',\n",
        "                  'who', 'when', 'where', 'why', 'how', 'very', 'too', 'not', 'so'}\n",
        "    \n",
        "    # Extract words (alphanumeric, 3+ characters)\n",
        "    words = re.findall(r'\\b[a-z]{3,}\\b', all_text)\n",
        "    \n",
        "    # Filter stop words\n",
        "    filtered_words = [w for w in words if w not in stop_words]\n",
        "    \n",
        "    # Count frequencies\n",
        "    word_counts = Counter(filtered_words)\n",
        "    \n",
        "    return word_counts.most_common(top_n)\n",
        "\n",
        "# Analyze positive feedback\n",
        "if len(positive_texts) > 0:\n",
        "    print(\"\\nMOST COMMON WORDS IN POSITIVE FEEDBACK (Top 15):\")\n",
        "    positive_words = get_word_frequencies(positive_texts, 15)\n",
        "    for word, count in positive_words:\n",
        "        print(f\"   {word:20s}: {count:>4} occurrences\")\n",
        "\n",
        "# Analyze improvement feedback\n",
        "if len(improvement_texts) > 0:\n",
        "    print(\"\\nMOST COMMON WORDS IN IMPROVEMENT FEEDBACK (Top 15):\")\n",
        "    improvement_words = get_word_frequencies(improvement_texts, 15)\n",
        "    for word, count in improvement_words:\n",
        "        print(f\"   {word:20s}: {count:>4} occurrences\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word Clouds (Optional)\n",
        "\n",
        "Generating word clouds if the wordcloud library is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate word clouds if available\n",
        "if WORDCLOUD_AVAILABLE and len(positive_texts) > 0:\n",
        "    try:\n",
        "        from wordcloud import WordCloud\n",
        "        \n",
        "        # Positive feedback word cloud\n",
        "        if len(positive_texts) > 5:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            \n",
        "            positive_text_combined = ' '.join(positive_texts)\n",
        "            wordcloud_pos = WordCloud(width=800, height=400, \n",
        "                                      background_color='white',\n",
        "                                      colormap='Greens',\n",
        "                                      max_words=100).generate(positive_text_combined)\n",
        "            \n",
        "            plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.title('Positive Feedback Word Cloud', fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            \n",
        "            plt.savefig(f'{FIGURE_DIR}/10_positive_feedback_wordcloud.png', \n",
        "                       dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "            print(f\"\u2713 Figure saved: {FIGURE_DIR}/10_positive_feedback_wordcloud.png\")\n",
        "            plt.close()\n",
        "        \n",
        "        # Improvement feedback word cloud\n",
        "        if len(improvement_texts) > 5:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            \n",
        "            improvement_text_combined = ' '.join(improvement_texts)\n",
        "            wordcloud_imp = WordCloud(width=800, height=400,\n",
        "                                      background_color='white',\n",
        "                                      colormap='Reds',\n",
        "                                      max_words=100).generate(improvement_text_combined)\n",
        "            \n",
        "            plt.imshow(wordcloud_imp, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.title('Improvement Feedback Word Cloud', fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            \n",
        "            plt.savefig(f'{FIGURE_DIR}/11_improvement_feedback_wordcloud.png',\n",
        "                       dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "            print(f\"\u2713 Figure saved: {FIGURE_DIR}/11_improvement_feedback_wordcloud.png\")\n",
        "            plt.close()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate word clouds: {e}\")\n",
        "else:\n",
        "    print(\"Word cloud generation skipped (library not available or insufficient data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"recommendations\"></a>\n",
        "## 10. Recommendations\n",
        "\n",
        "### Actionable Insights for Course Improvement\n",
        "\n",
        "Based on the comprehensive analysis, here are key recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overall Recommendations\n",
        "\n",
        "1. **Courses Needing Immediate Attention:**\n",
        "   - Review courses with low exam alignment scores\n",
        "   - Address courses with low re-enrolment intent\n",
        "   - Consider curriculum adjustments for courses with misaligned exams\n",
        "\n",
        "2. **Best Practices to Replicate:**\n",
        "   - Study \"hidden gem\" courses to understand what makes them accessible yet valuable\n",
        "   - Analyze \"challenging but rewarding\" courses to see how they maintain student engagement despite difficulty\n",
        "\n",
        "3. **Resource Allocation:**\n",
        "   - Provide additional support for highly difficult courses\n",
        "   - Ensure adequate preparation materials are available\n",
        "\n",
        "4. **Continuous Improvement:**\n",
        "   - Regularly collect and analyze student feedback\n",
        "   - Address common themes from open-ended responses\n",
        "   - Monitor changes in difficulty and satisfaction over time\n",
        "\n",
        "5. **Communication:**\n",
        "   - Set clear expectations about course difficulty\n",
        "   - Ensure exam content aligns with course materials\n",
        "   - Provide transparent information to help students make informed choices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"exports\"></a>\n",
        "## 11. Exports and Final Report\n",
        "\n",
        "### Summary of Generated Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of all generated files\n",
        "print(\"=\"*70)\n",
        "print(\" \" * 20 + \"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nGENERATED FILES:\\n\")\n",
        "\n",
        "print(\"1. Data Files:\")\n",
        "print(f\"   \u2713 {OUTPUT_DIR}/data_dictionary.csv\")\n",
        "print(f\"   \u2713 {OUTPUT_DIR}/processed_course_feedback.csv\")\n",
        "print(f\"   \u2713 {OUTPUT_DIR}/course_summary_statistics.csv\")\n",
        "print(f\"   \u2713 {OUTPUT_DIR}/correlation_matrix.csv\")\n",
        "print(f\"   \u2713 {OUTPUT_DIR}/top_courses_summary.csv\")\n",
        "print(f\"   \u2713 {OUTPUT_DIR}/key_insights.json\")\n",
        "\n",
        "print(\"\\n2. Visualizations:\")\n",
        "import os\n",
        "if os.path.exists(FIGURE_DIR):\n",
        "    figures = sorted([f for f in os.listdir(FIGURE_DIR) if f.endswith('.png')])\n",
        "    for fig in figures:\n",
        "        print(f\"   \u2713 {FIGURE_DIR}/{fig}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"All analyses completed successfully!\")\n",
        "print(\"Review the generated figures and data files for detailed insights.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Summary\n",
        "\n",
        "This comprehensive analysis has provided:\n",
        "\n",
        "1. \u2705 **Data Quality Report** - Validated and cleaned survey data\n",
        "2. \u2705 **Descriptive Statistics** - Comprehensive overview of all metrics\n",
        "3. \u2705 **Statistical Analysis** - Correlations, comparisons, and significance testing\n",
        "4. \u2705 **10+ Publication-Ready Visualizations** - High-quality figures for reports\n",
        "5. \u2705 **Key Insights** - Automated identification of top/bottom performers\n",
        "6. \u2705 **Text Analysis** - Examination of open-ended feedback\n",
        "7. \u2705 **Actionable Recommendations** - Evidence-based suggestions for improvement\n",
        "\n",
        "---\n",
        "\n",
        "**For questions or further analysis, refer to the generated output files and figures.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPH4MWs5bfETZ4zUwGMjChY",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}